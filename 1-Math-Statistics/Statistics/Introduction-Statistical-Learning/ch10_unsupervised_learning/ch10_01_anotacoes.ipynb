{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 10 - Aprendizado não supervisionado\n",
    "\n",
    "## O desafio do aprendizado não supervisionado\n",
    "\n",
    "Normalmente, técnicas de aprendizado não supervisionado são usadas para análise exploratória de dados.\n",
    "\n",
    "O livro ressalta sobre a dificuldade de avaliar os resultados obtidos pelos métodos de aprendizado não supervidionado pois não há um \"mecanismo universalmente aceito\". Porém, em [\"Introdução à mineração de dados\"](https://www.researchgate.net/publication/340903558_Introducao_a_Mineracao_de_Dados_Conceitos_Basicos_Algoritmos_e_Aplicacoes) são indicados algumas métricas. Ao final dessa seção, passaremos brevemente por algumas.\n",
    "\n",
    "## Análise dos componentes principais (PCA)\n",
    "\n",
    "O objetivo da **Análise dos componentes principais** ***(Principal Components Analysis)*** é reduzir a quantidade de dimensões em que suas observações estão sendo representadas.\n",
    "\n",
    "Cada dimensão criada a partir da técnica PCA é uma combinação linear contendo a maior variação de cada uma das dimensões originiais e, portanto, a primeira dimensão obtida, denominada PCA1, dessa forma será a combinação linear das maiores dispersões das dimenões originais. A segunda dimensão obtida, denominada PC2, será a combinação linear das maiores dispersões subsequentes das dimenões originais e será ortogonal a PCA1.\n",
    "\n",
    "### Outras técnicas de redução de dimensionalidade\n",
    "\n",
    "<p class=\"aligncenter\">\n",
    "    <img src=\"../figures/10_reducao_dimensionalidade.png\" alt=\"Mapa mental de técnicas de redução de dimensionalidade.\" width=\"850\">\n",
    "</p>\n",
    "<p style=\"text-align:center\"> Figura 1 - Mapa mental de técnicas de redução de dimensionalidade.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de agrupamento\n",
    "\n",
    "O objetivo dos métodos de agrupamenté encontrar subgrupos ou agrupamentos dentro de um conjunto de dados. Por subgrupo, entende-se como um conjunto de observações/registros com características similares entre si (intragrupo) e distintas entre os grupos.\n",
    "\n",
    "<p class=\"aligncenter\">\n",
    "    <img src=\"../figures/10_map_clustering.png\" alt=\"Mapa mental de modelos de agrupamento\" width=\"850\">\n",
    "</p>\n",
    "<p style=\"text-align:center\"> Figura 2 - Mapa mental de modelos de agrupamento.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "O algoritmo K-means divide o conjuntos de dados (observações) em *K* grupos, sendo K definido previamente. Toda observação pertence a apenas um grupo, ou seja:\n",
    "\n",
    "$$ C_{1} \\cup C_{2} \\cup...\\cup  C_{k} = \\left\\{1,..., n\\right\\} $$\n",
    "\n",
    "$$ C_{k} \\cap C_{k^´} = \\varnothing, \\forall k \\neq k^´$$\n",
    "\n",
    "Para determinar as observações que fazem parte de um mesmo grupo, essas observações devem estar o mais próximas possíveis. Uma das maneiras de fazer isso é minimizar a **variação intragrupo** ***(within-cluster variation)***:\n",
    "\n",
    "$$ Mín_{C_{1},...,C_{k}}\\left\\{ \\sum_{k=1}^K W(C_{k})\\right\\}$$\n",
    "\n",
    "Usando como medida de distanciamento a distância euclidiana, temos:\n",
    "\n",
    "$$ Mín_{C_{1},...,C_{k}}\\left\\{ \\sum_{k=1}^K \\frac{1}{\\left| C_{k} \\right|} \\sum_{i, i´ \\in C_{k}}\\sum_{j=1}^p (x_{ij}-x_{i´j})^2\\right\\}$$\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "- K-means é extremamente dependente da inicialização e, portanto, a solução encontrada pode não ser o mínimo global.\n",
    "- A quantidade K de grupos deve ser definida previamente.\n",
    "- Cada observação pertence a apenas um grupo, porém, há implementações em que o pertencimento a um grupo ou outro é dado por uma probabilidade. Isso é conhecido como ***Soft K-means***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierárquico\n",
    "\n",
    "Algoritmos de agrupamento hierárquico possuem duas abordagens de contrução ([Wikipedia](https://en.wikipedia.org/wiki/Hierarchical_clustering)):\n",
    "\n",
    "- Aglomerativa (\"Top-down\"): nessa abordagem, cada observação inicia como um subgrupo independente e vão formando grupos de acordo com sua maior similaridade / dissimilaridade.\n",
    "\n",
    "- Divisiva (\"Bottom-up\"): ao contrário da anterior, nessa abordagem todos os elementos iniciam dentro de um conjunto único e vão sendo divididos conforme sua menor similaridade / dissimilaridade.\n",
    "\n",
    "No ISL, diz que a abrodagem de construção para o algoritmo hierárquico mais comum é a **aglomerativa**. Seguiremos com essa aboradagem.\n",
    "\n",
    "Para a determinação de observações similares, usamos como medida de dissimilaridade a distância entre essas observações. Existem diversas medidas de distância, conforme tabela abaixo ([Wikipedia](https://en.wikipedia.org/wiki/Hierarchical_clustering)):\n",
    "\n",
    "|Tipo|Cálculo|\n",
    "|-|-|\n",
    "|Distância euclidiana<br>*(usaremos essa)*|${\\displaystyle \\|a-b\\|_{2}={\\sqrt {\\sum _{i}(a_{i}-b_{i})^{2}}}}$|\n",
    "|Quadrado da distância euclidiana|${\\displaystyle \\|a-b\\|_{2}^{2}=\\sum _{i}(a_{i}-b_{i})^{2}}$|\n",
    "|Distância de Manhattan|${\\displaystyle \\|a-b\\|_{1}=\\sum _{i}|a_{i}-b_{i}|}$|\n",
    "|Distância máxima|${\\displaystyle \\|a-b\\|_{\\infty }=\\max _{i}|a_{i}-b_{i}|}$|\n",
    "|Distância de Mahalanobis|${\\displaystyle {\\sqrt {(a-b)^{\\top }S^{-1}(a-b)}}} $ <br> onde S é a matriz de covariância|\n",
    "\n",
    "Após o cálculo das distâncias entre observações, precisamos definir a partir de qual ponto será medida a distância entre os grupos, isso é denominado função de ligação *(linkage)*. O livro indica quatro tipos de função de ligação:\n",
    "\n",
    "|Ligação (*Linkage*)|Descrição|\n",
    "|-|-|\n",
    "|Completa|Usa a **maior** distância entre pontos de dois grupos como medida de dissimilaridade entre grupos|\n",
    "|*Single*|Usa a **menor** distância entre pontos de dois grupos como medida de dissimilaridade entre grupos|\n",
    "|Média|Usa a **média** distância entre pontos de dois grupos como medida de dissimilaridade entre grupos|\n",
    "|Centróide|Usa a **distância entre os centróides** dos dois grupos como medida de dissimilaridade entre grupos|\n",
    "\n",
    "A maneira mais usual de observar o resultado desse algoritmo é a representação gráfica conhecida como dendrograma:\n",
    "\n",
    "<p class=\"aligncenter\">\n",
    "    <img src=\"../figures/10_dendrogram.jpg\" alt=\"Dendrograma\" width=\"600\">\n",
    "</p>\n",
    "<p style=\"text-align:center\"> Figura 3 - Exemplo de dendrograma. Fonte: ISL.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas possíveis na aplicação de agrupamentos\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "309px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
